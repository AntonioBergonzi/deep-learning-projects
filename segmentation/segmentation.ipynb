{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvZvl3TAQNsv"
      },
      "source": [
        "\n",
        "# Description of the procedure that we have followed\n",
        "\n",
        "First of all, to train our model we decided to use BipBip Haricot to concentrate our efforts on one class. We started from the Notebook that we saw during the lessons, and tried to enhance its performance through parameter tuning. After we realised that we couldn't do better than 0.25-0.30 in meanIoU, we changed our approach. Our idea was to use a more classic encoder-decoder approach, and we did it, but again our performance was not improving as much as we hoped.<br>\n",
        "We then implemented the UNet that we saw at lesson, and after we managed to reach around 0.40-0.45 in meanIoU, we decided to insert an Attention Layer and then perform Transfer Learning. In order to do this, we substituted the encoder part of the UNet with a vgg-16 net, with non-trainable layers. However, we could not increase a lot the meanIoU, so we decided to implement tiling. <br>\n",
        "Since we didn't want to lose too much time before training (which is time consuming by definition), after we implemented the function to divide the images in 256x256 patches, we generated all the ones that we needed for the training and the validation set. This means that the attached code has to be modified in order to create them at least at the first run. How to do it is explained in the notebook before the Data Augmentation cell.<br>\n",
        "However, we realised that the UNet built with vgg-16 as encorder had too many parameters (around 13 millions) and by visualising our results we saw that the network was too biased towards the background class. So, we came back to the previous model, increasing the depth of our UNet. At the end, we managed to reach a meanIoU of 0.60-0.65.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDqLYGQGLc5s"
      },
      "source": [
        "# Preliminar Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5FnwOggnFIl"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOBDSA9BnFIx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for random operations. \n",
        "# This let our experiments to be reproducible. \n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urzY3C61_BmT"
      },
      "outputs": [],
      "source": [
        "FREEZE_UNTIL_TL = 5\n",
        "bs = 1\n",
        "lr = 5e-6\n",
        "patches = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe7j5-KbmoZy",
        "outputId": "2ecc7b38-3e9d-4867-de2c-dd7b04f348e3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-0p2u4Rmte-",
        "outputId": "fa6cb08d-2a36-4385-c3f5-53cf7fe9246c"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/dataset/second_challenge.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMFrinBTu4EQ"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/Development_Dataset/Training/Bipbip/Haricot/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne6TV7cYutr7"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import random\n",
        "\n",
        "def create_file_txt(path, percentage_validation):\n",
        "  files = [f[:-4]  for f in listdir(path) if isfile(join(path, f)) and f.lower().endswith('.jpg')]\n",
        "  files_validation = random.sample(files, int(len(files)*percentage_validation))\n",
        "  for file in files_validation:\n",
        "      files.remove(file)\n",
        "  with open(os.path.join(path, \"train.txt\"), 'w') as f:\n",
        "    for file in files:\n",
        "        f.write(file+'\\n')  \n",
        "  with open(os.path.join(path, \"val.txt\"), 'w') as f:\n",
        "    for file in files_validation:\n",
        "        f.write(file+'\\n') \n",
        "  \n",
        "  return\n",
        "\n",
        "create_file_txt(base_dir+'Images',0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH3lzlVqnFI2"
      },
      "source": [
        "# Image Segmentation\n",
        "## Build segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpHZzWHRk-kI"
      },
      "outputs": [],
      "source": [
        "def get_patches_img_array(img):\n",
        "    ksize_rows = 256\n",
        "    ksize_cols = 256\n",
        "    strides_rows = 256\n",
        "    strides_cols = 256\n",
        "\n",
        "    # The size of sliding window\n",
        "    ksizes = [1, ksize_rows, ksize_cols, 1] \n",
        "\n",
        "    # How far the centers of 2 consecutive patches are in the image\n",
        "    strides = [1, strides_rows, strides_cols, 1]\n",
        "\n",
        "    # sample pixel consecutively\n",
        "    rates = [1, 1, 1, 1] \n",
        "\n",
        "    # padding algorithm to used\n",
        "    padding='VALID'\n",
        "\n",
        "    patches = tf.image.extract_patches([img], ksizes, strides, rates, padding)\n",
        "\n",
        "    return patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH7PAt4dk-kJ"
      },
      "outputs": [],
      "source": [
        "def rescale(arr):\n",
        "    arr_min = arr.min()\n",
        "    arr_max = arr.max()\n",
        "    return (arr - arr_min) / (arr_max - arr_min)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IGMbeOCk-kK"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def create_images(path, dataset_dir, file):\n",
        "    with open(file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "    \n",
        "    subset_filenames = []\n",
        "    for line in lines:\n",
        "      subset_filenames.append(line.strip()) \n",
        "    \n",
        "    \n",
        "    for curr_filename in subset_filenames:\n",
        "        print(curr_filename)\n",
        "        img = Image.open(os.path.join(dataset_dir, 'Images', curr_filename + '.jpg'))\n",
        "        mask = Image.open(os.path.join(dataset_dir, 'Masks', curr_filename + '.png'))\n",
        "        img_arr = np.array(img)\n",
        "        mask_arr = np.array(mask)\n",
        "\n",
        "\n",
        "        img_patches = get_patches_img_array(img_arr)\n",
        "        mask_patches = get_patches_img_array(mask_arr)\n",
        "        for i in range(img_patches.shape[1]):\n",
        "            for j in range(img_patches.shape[2]):\n",
        "                mask_patch = mask_patches[0,i,j,]\n",
        "                img_patch = img_patches[0,i,j,]\n",
        "                \n",
        "                mask_patch = tf.reshape(mask_patch, [256, 256, 3])\n",
        "                img_patch = tf.reshape(img_patch, [256, 256, 3])\n",
        "\n",
        "                name = curr_filename + \"_\"+str(i)+\"_\"+str(j)\n",
        "                print(\"\\t\" + name)\n",
        "                \n",
        "                \n",
        "                \n",
        "                img_patch = img_patch.numpy()\n",
        "\n",
        "                im = Image.fromarray((img_patch).astype(np.uint8),'RGB')\n",
        "                \n",
        "                mask_patch = mask_patch.numpy()\n",
        "                msk = Image.fromarray(mask_patch.astype(np.uint8), 'RGB')\n",
        "                \n",
        "                im.save(os.path.join(path, name+\".jpg\" ))\n",
        "                msk.save(os.path.join(path, name+\".png\" ))\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V8FLMTFAhVD"
      },
      "source": [
        "In order to create patches, you will have to call twice the following function (one for the training set and one for the validation set). The parameters have to be initialised as you want, and are\n",
        "\n",
        "*   The path where you want to save your patches\n",
        "*   The directory in which the dataset is saved\n",
        "*   The file txt that lists the training and validation files respectively\n",
        "\n",
        "Remember to change in the class \"CustomDatasetPatches\" of the notebook the directory in which you put the patches (at the moment, we called them \"/content/drive/MyDrive/ANN second challenge/kaggle/working/train\" and \"/content/drive/MyDrive/ANN second challenge/kaggle/working/valid\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUuHcI9l5r2m"
      },
      "outputs": [],
      "source": [
        "#create_images(path_to_save_patches, dataset_dir, file_txt_to_read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N88wG50nFI3"
      },
      "outputs": [],
      "source": [
        "# ImageDataGenerator\n",
        "# ------------------\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "apply_data_augmentation = True\n",
        "\n",
        "# Create training ImageDataGenerator object\n",
        "# We need two different generators for images and corresponding masks\n",
        "if apply_data_augmentation:\n",
        "    img_data_gen = ImageDataGenerator(rotation_range=10,\n",
        "                                      width_shift_range=10,\n",
        "                                      height_shift_range=10,\n",
        "                                      zoom_range=0.3,\n",
        "                                      horizontal_flip=True,\n",
        "                                      vertical_flip=True,\n",
        "                                      fill_mode='reflect')\n",
        "    mask_data_gen = ImageDataGenerator(rotation_range=10,\n",
        "                                       width_shift_range=10,\n",
        "                                       height_shift_range=10,\n",
        "                                       zoom_range=0.3,\n",
        "                                       horizontal_flip=True,\n",
        "                                       vertical_flip=True,\n",
        "                                       fill_mode='reflect')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsieZk4aKhm6"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "class CustomDatasetPatches(tf.keras.utils.Sequence):\n",
        "\n",
        "  \"\"\"\n",
        "    CustomDataset inheriting from tf.keras.utils.Sequence.\n",
        "\n",
        "    3 main methods:\n",
        "      - __init__: save dataset params like directory, filenames..\n",
        "      - __len__: return the total number of samples in the dataset\n",
        "      - __getitem__: return a sample from the dataset\n",
        "\n",
        "    Note: \n",
        "      - the custom dataset return a single sample from the dataset. Then, we use \n",
        "        a tf.data.Dataset object to group samples into batches.\n",
        "      - in this case we have a different structure of the dataset in memory. \n",
        "        We have all the images in the same folder and the training and validation splits\n",
        "        are defined in text files.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \n",
        "               preprocessing_function=None, out_shape=[256, 256]):\n",
        "    if which_subset == 'training':\n",
        "      self.path = '/content/drive/MyDrive/ANN second challenge/kaggle/working/train'\n",
        "    elif which_subset == 'validation':\n",
        "      self.path = '/content/drive/MyDrive/ANN second challenge/kaggle/working/valid'\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    \n",
        "    \n",
        "    self.subset_filenames = [f[:-4] for f in os.listdir(self.path) if f.endswith(\".jpg\")]\n",
        "    \n",
        "    \n",
        "    self.which_subset = which_subset\n",
        "    \n",
        "    self.img_generator = img_generator\n",
        "    self.mask_generator = mask_generator\n",
        "    self.preprocessing_function = preprocessing_function\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  def __len__(self):\n",
        "        return len(self.subset_filenames)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Read Image\n",
        "    curr_filename = self.subset_filenames[index]\n",
        "    \n",
        "    img = Image.open(os.path.join(self.path, curr_filename + '.jpg'))\n",
        "    mask = Image.open(os.path.join(self.path, curr_filename + '.png'))\n",
        "\n",
        "    \n",
        "    img_arr = np.array(img)\n",
        "    mask_arr = np.array(mask)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    if self.which_subset == 'training':\n",
        "      if self.img_generator is not None and self.mask_generator is not None:\n",
        "        # Perform data augmentation\n",
        "        # We can get a random transformation from the ImageDataGenerator using get_random_transform\n",
        "        # and we can apply it to the image using apply_transform\n",
        "        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n",
        "        mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n",
        "        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n",
        "        out_mask = np.zeros_like(mask_arr)\n",
        "        for c in np.unique(mask_arr):\n",
        "          if c > 0:\n",
        "            curr_class_arr = np.float32(mask_arr == c)\n",
        "            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n",
        "            # from [0, 1] to {0, 1}\n",
        "            curr_class_arr = np.uint8(curr_class_arr)\n",
        "            # recover original class\n",
        "            curr_class_arr = curr_class_arr * c \n",
        "            out_mask += curr_class_arr\n",
        "    \n",
        "        mask_arr = out_mask\n",
        "        \n",
        "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n",
        "    \n",
        "      # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 124, 18], axis=-1))] = 0\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n",
        "    \n",
        "    mask_arr = new_mask_arr\n",
        "\n",
        "    mask_arr = np.expand_dims(mask_arr, -1)    \n",
        "    \n",
        "    if self.preprocessing_function is not None:\n",
        "        img_arr = self.preprocessing_function(img_arr)\n",
        "\n",
        "    return img_arr, np.float32(mask_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jd6PTgzk-kM"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "\n",
        "  \"\"\"\n",
        "    CustomDataset inheriting from tf.keras.utils.Sequence.\n",
        "\n",
        "    3 main methods:\n",
        "      - __init__: save dataset params like directory, filenames..\n",
        "      - __len__: return the total number of samples in the dataset\n",
        "      - __getitem__: return a sample from the dataset\n",
        "\n",
        "    Note: \n",
        "      - the custom dataset return a single sample from the dataset. Then, we use \n",
        "        a tf.data.Dataset object to group samples into batches.\n",
        "      - in this case we have a different structure of the dataset in memory. \n",
        "        We have all the images in the same folder and the training and validation splits\n",
        "        are defined in text files.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \n",
        "               preprocessing_function=None, out_shape=[256, 256]):\n",
        "    if which_subset == 'training':\n",
        "      subset_file = os.path.join(dataset_dir, 'Images', 'train.txt')\n",
        "    elif which_subset == 'validation':\n",
        "      subset_file = os.path.join(dataset_dir, 'Images', 'val.txt')\n",
        "\n",
        "    with open(subset_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "    \n",
        "    subset_filenames = []\n",
        "    for line in lines:\n",
        "      subset_filenames.append(line.strip()) \n",
        "\n",
        "    self.which_subset = which_subset\n",
        "    self.dataset_dir = dataset_dir\n",
        "    self.subset_filenames = subset_filenames\n",
        "    self.img_generator = img_generator\n",
        "    self.mask_generator = mask_generator\n",
        "    self.preprocessing_function = preprocessing_function\n",
        "    self.out_shape = out_shape\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.subset_filenames)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Read Image\n",
        "    curr_filename = self.subset_filenames[index]\n",
        "    img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.jpg'))\n",
        "    mask = Image.open(os.path.join(self.dataset_dir, 'Masks', curr_filename + '.png'))\n",
        "\n",
        "    # Resize image and mask\n",
        "    img = img.resize(self.out_shape)\n",
        "    mask = mask.resize(self.out_shape, resample=Image.NEAREST)\n",
        "    \n",
        "    img_arr = np.array(img)\n",
        "    mask_arr = np.array(mask)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    if self.which_subset == 'training':\n",
        "      if self.img_generator is not None and self.mask_generator is not None:\n",
        "        # Perform data augmentation\n",
        "        # We can get a random transformation from the ImageDataGenerator using get_random_transform\n",
        "        # and we can apply it to the image using apply_transform\n",
        "        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n",
        "        mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n",
        "        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n",
        "        out_mask = np.zeros_like(mask_arr)\n",
        "        for c in np.unique(mask_arr):\n",
        "          if c > 0:\n",
        "            curr_class_arr = np.float32(mask_arr == c)\n",
        "            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n",
        "            # from [0, 1] to {0, 1}\n",
        "            curr_class_arr = np.uint8(curr_class_arr)\n",
        "            # recover original class\n",
        "            curr_class_arr = curr_class_arr * c \n",
        "            out_mask += curr_class_arr\n",
        "    \n",
        "        mask_arr = out_mask\n",
        "        \n",
        "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n",
        "    \n",
        "      # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 124, 18], axis=-1))] = 0\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n",
        "    \n",
        "    mask_arr = new_mask_arr\n",
        "\n",
        "    mask_arr = np.expand_dims(mask_arr, -1)    \n",
        "    \n",
        "    if self.preprocessing_function is not None:\n",
        "        img_arr = self.preprocessing_function(img_arr)\n",
        "\n",
        "    return img_arr, np.float32(mask_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyrdiIh_PWjB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "img_h = 256\n",
        "img_w = 256\n",
        "\n",
        "#if you are using vgg, add in the CustomDataset function \"preprocessing_function=preprocess_input\"\n",
        "if patches :\n",
        "    dataset = CustomDatasetPatches(base_dir, 'training', \n",
        "                            img_generator=img_data_gen, \n",
        "                            mask_generator=mask_data_gen, \n",
        "                            out_shape=[img_h,img_w]\n",
        "                            )\n",
        "\n",
        "    dataset_valid = CustomDatasetPatches(base_dir, \n",
        "                                  'validation', \n",
        "                                  out_shape=[img_h,img_w]\n",
        "                            )\n",
        "\n",
        "\n",
        "else :\n",
        "    dataset = CustomDataset(base_dir, 'training', \n",
        "                            img_generator=img_data_gen, \n",
        "                            mask_generator=mask_data_gen, \n",
        "                            out_shape=[img_h,img_w]\n",
        "                            )\n",
        "\n",
        "    dataset_valid = CustomDataset(base_dir, \n",
        "                                  'validation', \n",
        "                                  out_shape=[img_h,img_w]\n",
        "                                  )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usz5SKPeQrOE"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w,1]))\n",
        "\n",
        "train_dataset = train_dataset.batch(bs)\n",
        "\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w,1]))\n",
        "valid_dataset = valid_dataset.batch(bs)\n",
        "\n",
        "valid_dataset = valid_dataset.repeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOBabUbmnFJE"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "colors = {}\n",
        "colors[0] = [0,0,0]\n",
        "colors[1] = [255,124,18]\n",
        "colors[2] = [255,255,255]\n",
        "colors[3] = [216, 67, 82]\n",
        "\n",
        "\n",
        "\n",
        "iterator = iter(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "9eunbPwWqPnB",
        "outputId": "91fbba02-3dae-499b-f5ac-3820dd37856c"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "augmented_img, target = next(iterator)\n",
        "\n",
        "\n",
        "augmented_img = augmented_img[0]   # First element\n",
        "\n",
        "target = np.array(target[0,...,0])   # First element (squeezing channel dimension)\n",
        "\n",
        "\n",
        "\n",
        "target_img = np.zeros([img_h, img_w, 3])\n",
        "\n",
        "\n",
        "for i in range(0, 3):\n",
        "    target_img[np.where(target == i)] = np.array(colors[i])[:3]\n",
        "\n",
        "ax[0].imshow(np.uint8(augmented_img))\n",
        "ax[1].imshow(np.uint8(target_img))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIdSTI8yk-kQ"
      },
      "source": [
        "# Creating model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SBk5POCk-kR"
      },
      "outputs": [],
      "source": [
        "def attention(query, value):\n",
        "\n",
        "    # CNN layer.\n",
        "    cnn_layer = tf.keras.layers.Conv1D(\n",
        "        filters=100,\n",
        "        kernel_size=4,\n",
        "        # Use 'same' padding so outputs have the same shape as inputs.\n",
        "        padding='same')\n",
        "    \n",
        "    # Query encoding of shape [batch_size, Tq, filters].\n",
        "    query_seq_encoding = cnn_layer(query)\n",
        "    #print(query_seq_encoding.shape)\n",
        "    \n",
        "    # Value encoding of shape [batch_size, Tv, filters].\n",
        "    value_seq_encoding = cnn_layer(value)\n",
        "    #print(value_seq_encoding.shape)\n",
        "    \n",
        "    \n",
        "    # Query-value attention of shape [batch_size, Tq, filters].\n",
        "    query_value_attention_seq = tf.keras.layers.Attention()(\n",
        "        [query_seq_encoding, value_seq_encoding])\n",
        "\n",
        "    return query_value_attention_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elmWDgDrk-kS"
      },
      "outputs": [],
      "source": [
        "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    c = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
        "    c = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    p = tf.keras.layers.MaxPool2D((2, 2), (2, 2))(c)\n",
        "    return c, p\n",
        "\n",
        "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    us = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "    att = attention(us, skip)\n",
        "    concat = tf.keras.layers.Concatenate()([att, us])\n",
        "    c = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n",
        "    c = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    return c\n",
        "\n",
        "def bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    c = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
        "    c = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    return c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTOtxhalk-kS"
      },
      "outputs": [],
      "source": [
        "def UNetAtt(f = [16, 32, 64, 128, 256]):\n",
        "    \n",
        "    inputs = tf.keras.layers.Input((256, 256, 3))\n",
        "    \n",
        "    p0 = inputs\n",
        "    \n",
        "    # c is the output of the convolution block while p is the ouput of the pooling layer\n",
        "    c1, p1 = down_block(p0, f[1]) # 256x256x3  -> 128x128x32\n",
        "    c2, p2 = down_block(p1, f[2]) # 128x128x32 -> 64x64x64\n",
        "    c3, p3 = down_block(p2, f[3]) # 64x64x64   -> 32x32x128\n",
        "    c4, p4 = down_block(p3, f[4]) # 32x32x128  -> 16x16x256\n",
        "    \n",
        "    \n",
        "    bn = bottleneck(p4, f[4]) # convolution layer\n",
        "    \n",
        "    u1 = up_block(bn, c4, f[3]) # 16x16x256  -> 32x32x128 \n",
        "    u2 = up_block(u1, c3, f[2]) # 32x32x128  -> 64x64x64\n",
        "    u3 = up_block(u2, c2, f[1]) # 64x64x64   -> 128x128x32\n",
        "    u4 = up_block(u3, c1, f[0]) # 128x128x32 -> 256x256x16\n",
        "    \n",
        "    outputs = tf.keras.layers.Conv2D(3, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n",
        "    model = tf.keras.models.Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrN-peX3k-kT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "def UNetAttVGG():\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=(256, 256, 3), name=\"input_image\")\n",
        "    \n",
        "    vgg = tf.keras.applications.VGG16(input_tensor=inputs, weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "    for layer in vgg.layers:\n",
        "        layer.trainable = False \n",
        "\n",
        "    encoder_output = vgg.get_layer(\"block5_pool\").output    \n",
        "\n",
        "\n",
        "    \n",
        "    # block1 : 256x256x3  -> 128x128x64\n",
        "    # block2 : 128x128x64 -> 64x64x128\n",
        "    # block3 : 64x64x128  -> 32x32x256\n",
        "    # block4 : 32x32x256  -> 16x16x512\n",
        "    # block5 : 16x16x512  -> 8x8x512\n",
        "\n",
        "    \n",
        "    bn = bottleneck(encoder_output, 512) # convolution\n",
        "\n",
        "    \n",
        "    u2 = up_block(bn, vgg.get_layer(\"block5_conv3\").output, 512) \n",
        "    u3 = up_block(u2, vgg.get_layer(\"block4_conv3\").output, 256) \n",
        "    u4 = up_block(u3, vgg.get_layer(\"block3_conv2\").output, 128) \n",
        "    u5 = up_block(u4, vgg.get_layer(\"block2_conv2\").output, 64) \n",
        "    u6 = up_block(u5, vgg.get_layer(\"block1_conv2\").output, 32)\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(3, (1, 1), padding=\"same\", activation=\"softmax\")(u6)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEWA3EvagjPL",
        "outputId": "4dddc55c-617e-45c9-d6cd-3ced16644af0"
      },
      "outputs": [],
      "source": [
        "model = UNetAtt()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGzh16WTnFJW"
      },
      "source": [
        "## Prepare the model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MlmYGVMnFJW"
      },
      "outputs": [],
      "source": [
        "# Optimization params\n",
        "# -------------------\n",
        "\n",
        "# Loss\n",
        "# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy() \n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "# -------------------\n",
        "\n",
        "# Here we define the intersection over union for each class in the batch.\n",
        "# Then we compute the final iou as the mean over classes\n",
        "def meanIoU(y_true, y_pred):\n",
        "    # get predicted class from softmax\n",
        "    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n",
        "\n",
        "    per_class_iou = []\n",
        "\n",
        "    for i in range(1,3): # exclude the background class 0\n",
        "      # Get prediction and target related to only a single class (i)\n",
        "      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n",
        "      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n",
        "      intersection = tf.reduce_sum(class_true * class_pred)\n",
        "      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n",
        "    \n",
        "      iou = (intersection + 1e-7) / (union + 1e-7)\n",
        "      per_class_iou.append(iou)\n",
        "\n",
        "    return tf.reduce_mean(per_class_iou)\n",
        "\n",
        "# Validation metrics\n",
        "# ------------------\n",
        "metrics = [meanIoU]\n",
        "# ------------------\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA6D_TBknFJZ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "3XiwaKZhnFJa",
        "outputId": "b590661c-5f70-4215-d210-985fd9412869",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "exps_dir = os.path.join(cwd, 'drive/My Drive/Keras4/', 'multiclass_segmentation_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "model_name = 'CNN'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "    \n",
        "callbacks = []\n",
        "\n",
        "# Early Stopping\n",
        "# --------------\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping( monitor='val_loss', \n",
        "                                                    patience=10, \n",
        "                                                   mode='min',\n",
        "                                                  restore_best_weights = True)\n",
        "    callbacks.append(es_callback)\n",
        "\n",
        "\n",
        "model.fit(x=train_dataset,\n",
        "          epochs=100,  #### set repeat in training dataset\n",
        "          steps_per_epoch=len(dataset),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(dataset_valid),\n",
        "          callbacks=callbacks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_fpqjcvk-kV"
      },
      "outputs": [],
      "source": [
        "#de-comment if you wish to save the weights\n",
        "\n",
        "#model.save_weights('/content/drive/My Drive/my_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgwyeMnGN0ub"
      },
      "source": [
        "# Generate submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyfhzJLWDE15"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def read_rgb_mask(img_path):\n",
        "    '''\n",
        "    img_path: path to the mask file\n",
        "    Returns the numpy array containing target values\n",
        "    '''\n",
        "\n",
        "    mask_img = Image.open(img_path)\n",
        "    mask_arr = np.array(mask_img)\n",
        "\n",
        "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n",
        "\n",
        "    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 124, 18], axis=-1))] = 0\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n",
        "\n",
        "    return new_mask_arr\n",
        "\n",
        "def rle_encode(img):\n",
        "    '''\n",
        "    img: numpy array, 1 - foreground, 0 - background\n",
        "    Returns run length as string formatted\n",
        "    '''\n",
        "    pixels = img.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "V-VkZvxCIV9Z",
        "outputId": "8d31a36c-5ef4-4676-d0cd-84be4e903439"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "if patches :\n",
        "    submission_dict = {} \n",
        "    test_path = '/content/Development_Dataset/Test_Dev'\n",
        "    for dirs in os.listdir(test_path):\n",
        "        dir = os.path.join(test_path, dirs)\n",
        "        for subdir in os.listdir(dir):\n",
        "            files_list = os.path.join(dir, subdir+'/Images')\n",
        "            for file in os.listdir(files_list):\n",
        "                print(\"Currently segmenting image \" + file)\n",
        "                if file.endswith('.jpg') or file.endswith('.png'):\n",
        "                    img_name = file[:-4]\n",
        "                else:\n",
        "                    print(\"NOT AN IMAGE!\")\n",
        "                img_path = os.path.join(files_list, file)\n",
        "                img = image.load_img(img_path, target_size=(1536, 2048))\n",
        "\n",
        "\n",
        "                img_array = image.img_to_array(img)\n",
        "                img_batch = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "\n",
        "                img_patches = get_patches_img_array(img_array)\n",
        "\n",
        "                full_mask_arr = None\n",
        "\n",
        "                for i in range(img_patches.shape[1]):\n",
        "\n",
        "                    line_mask = None\n",
        "                    for j in range(img_patches.shape[2]):\n",
        "\n",
        "                        img_preprocessed = img_patches[0,i,j,]\n",
        "                        img_preprocessed = tf.reshape(img_preprocessed, [256, 256, 3]).numpy().astype(np.uint8)\n",
        "\n",
        "                        prediction = model.predict(x=tf.expand_dims(img_preprocessed,0))\n",
        "\n",
        "                        mask_arr = np.array(prediction)\n",
        "                        mask_arr = tf.argmax(mask_arr, -1)[0, ...] # (256, 256)\n",
        "\n",
        "                        # horizontal stack -> axis = 1\n",
        "                        if line_mask == None:\n",
        "                            line_mask = mask_arr\n",
        "                        else:\n",
        "                            line_mask = tf.concat([line_mask, mask_arr], axis=1)\n",
        "\n",
        "                    # vertical stack -> axis = 0\n",
        "                    if full_mask_arr == None:\n",
        "                        full_mask_arr = line_mask\n",
        "                    else:\n",
        "                        full_mask_arr = tf.concat([full_mask_arr, line_mask], axis=0)\n",
        "\n",
        "                full_mask_arr = np.array(full_mask_arr)\n",
        "                submission_dict[img_name] = {}\n",
        "                submission_dict[img_name]['shape'] = full_mask_arr.shape\n",
        "                submission_dict[img_name]['team'] = dirs\n",
        "                submission_dict[img_name]['crop'] = subdir\n",
        "                submission_dict[img_name]['segmentation'] = {}\n",
        "\n",
        "                rle_encoded_crop = rle_encode(full_mask_arr == 1)\n",
        "                rle_encoded_weed = rle_encode(full_mask_arr == 2)\n",
        "\n",
        "                submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n",
        "                submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n",
        "\n",
        "    with open(os.path.join('/content/drive/My Drive', 'submission.json'), 'w') as f:\n",
        "        json.dump(submission_dict, f)\n",
        "\n",
        "else:\n",
        "    submission_dict = {} \n",
        "    test_path = '/content/Development_Dataset/Test_Dev'\n",
        "    for dirs in os.listdir(test_path):\n",
        "      dir = os.path.join(test_path, dirs)\n",
        "      for subdir in os.listdir(dir):\n",
        "        files_list = os.path.join(dir, subdir+'/Images')\n",
        "        for file in os.listdir(files_list):\n",
        "          if file.endswith('.jpg') or file.endswith('.png'):\n",
        "            img_name = file[:-4]\n",
        "          else:\n",
        "            print(\"NOT AN IMAGE!\")\n",
        "\n",
        "          img_path = os.path.join(files_list, file)\n",
        "          img = image.load_img(img_path, target_size=(1536, 2048))\n",
        "          img_array = image.img_to_array(img)\n",
        "          img_batch = np.expand_dims(img_array, axis=0)\n",
        "          img_preprocessed = img_batch\n",
        "\n",
        "          prediction = model.predict(img_preprocessed)\n",
        "\n",
        "          mask_arr = tf.argmax(prediction, -1)[0, ...] # (256, 256)\n",
        "          mask_arr = np.array(mask_arr)\n",
        "\n",
        "          submission_dict[img_name] = {}\n",
        "          submission_dict[img_name]['shape'] = mask_arr.shape\n",
        "          submission_dict[img_name]['team'] = dirs\n",
        "          submission_dict[img_name]['crop'] = subdir\n",
        "          submission_dict[img_name]['segmentation'] = {}\n",
        "\n",
        "          rle_encoded_crop = rle_encode(mask_arr == 1)\n",
        "          rle_encoded_weed = rle_encode(mask_arr == 2)\n",
        "\n",
        "          submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n",
        "          submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n",
        "\n",
        "    with open(os.path.join('/content/drive/My Drive', 'submission.json'), 'w') as f:\n",
        "        json.dump(submission_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuVw1q_NnFJh"
      },
      "source": [
        "## Visualisation of the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo1JbCO5nFJh",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "iterator = iter(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oXD2wy2k-kX"
      },
      "outputs": [],
      "source": [
        "complete_image = Image.new('RGB',(256*8, 256*6), (255,255,255))\n",
        "for i in range(6):\n",
        "    for j in range(8):\n",
        "        image,_ = next(iterator)\n",
        "        image = image[0].numpy()\n",
        "        im = Image.fromarray((image).astype(np.uint8),'RGB')\n",
        "        complete_image.paste(im,(256*j, 256*i))\n",
        "    \n",
        "complete_image.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "HLIb-SlMk-kX",
        "outputId": "8ee42a23-b626-439d-fc29-85f9bc43f4f2"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(8, 8))\n",
        "fig.show()\n",
        "ax[0].imshow(np.uint8(complete_image))\n",
        "\n",
        "\n",
        "fig.canvas.draw()\n",
        "time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "M75UjH7xxS7J",
        "outputId": "4456d2f6-2cf1-4157-ca80-b94a7974f934"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(8, 8))\n",
        "fig.show()\n",
        "image, target = next(iterator)\n",
        "\n",
        "colors = {}\n",
        "colors[0] = [0,0,0]\n",
        "colors[1] = [255,124,18]\n",
        "colors[2] = [255,255,255]\n",
        "colors[3] = [216, 67, 82]\n",
        "\n",
        "\n",
        "\n",
        "image = image[0]\n",
        "target = target[0, ..., 0]\n",
        "\n",
        "out_sigmoid = model.predict(x=tf.expand_dims(image, 0))\n",
        "\n",
        "# Get predicted class as the index corresponding to the maximum value in the vector probability\n",
        "# predicted_class = tf.cast(out_sigmoid > score_th, tf.int32)\n",
        "# predicted_class = predicted_class[0, ..., 0]\n",
        "predicted_class = tf.argmax(out_sigmoid, -1)\n",
        "\n",
        "#out_sigmoid.shape\n",
        "\n",
        "predicted_class = predicted_class[0, ...]\n",
        "\n",
        "# Assign colors (just for visualization)\n",
        "target_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
        "prediction_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
        "\n",
        "target_img[np.where(target == 0)] = [0, 0, 0]\n",
        "\n",
        "\n",
        "for i in range(0, 3):\n",
        "  target_img[np.where(target == i)] = np.array(colors[i])[:3]\n",
        "\n",
        "prediction_img[np.where(predicted_class == 0)] = [0, 0, 0]\n",
        "for i in range(0, 3):\n",
        "  prediction_img[np.where(predicted_class == i)] = np.array(colors[i])[:3] \n",
        "\n",
        "ax[0].imshow(np.uint8(image))\n",
        "ax[1].imshow(np.uint8(target_img))\n",
        "ax[2].imshow(np.uint8(prediction_img))\n",
        "\n",
        "fig.canvas.draw()\n",
        "time.sleep(1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "segmentation_kaggle.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
